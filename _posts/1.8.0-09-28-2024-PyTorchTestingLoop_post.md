# PyTorch Testing Loop

```
# Setup empty lists to keep track of model progress
epoch_count = []
train_loss_values = []
test_loss_values = []

# Pass the data through the model for a number of epochs (i.e. 100 epochs)
for epoch in range(epochs)
  ### Training loop code here ###
  ### Testing Starts ###
  # Put the model in evaluation model
  model.eval()
  # Turn on inference mode context manager
  with torch.inference():
    # 1. Forward pass on test data
    test_pred = model(X_test
    # 2. Calculate the loss on test data
    test_loss = loss_fn(test_pred, y_test)
# Print out what's happening every 10 epochs
if epoch % 10 == 0:
  epoch.count.append(epoch)
  train_loss_values.append(loss)
  test_loss_values.append(test_loss)
  print(f"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss}")
```

## Breakdown of each step
  1. Create empty lists for storing useful values (helpful tracking model progress)
  2. Tell the model we want to evaluate rather than train (this turns off functionality used for training but not evaluation)
  3. Turn on torch.inference_model() context manager to disable functionality such as gradient tracking for inference (gradient tracking not needed for inference)
  4. Pass the test data through the model (this will call the model's implemented forward() method)
  5. Calculate the test loss value (how wrong the model's predictions are on the test dataset, lower is better)
  6. Display information outputs for how the model is doing during training/testing every ~10 epochs (note: what gets printed out here can be adjusted for specific problems)
